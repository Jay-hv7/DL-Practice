{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0uz77uZT+PgxlygnobVGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jay-hv7/DL-Practice/blob/main/Torch_profiling_CUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JVx_LM9riJ0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bhVka7mWiNXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n"
      ],
      "metadata": {
        "id": "HiSNZAXNiULd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiCYJWHpjLsV",
        "outputId": "a6d6aae1-2ca3-4a41-e50f-26c645b94811"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 104M/104M [00:00<00:00, 211MB/s] \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Inception3(\n",
              "  (Conv2d_1a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2b_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Conv2d_3b_1x1): BasicConv2d(\n",
              "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_4a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Mixed_5b): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5c): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5d): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6a): InceptionB(\n",
              "    (branch3x3): BasicConv2d(\n",
              "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6b): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6c): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6d): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6e): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (AuxLogits): InceptionAux(\n",
              "    (conv0): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv1): BasicConv2d(\n",
              "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              "  (Mixed_7a): InceptionD(\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7b): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7c): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try:\n",
        "  urllib.URLopener().retrieve(url, filename)\n",
        "except:\n",
        "  urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "PQMIZkGojiNS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename\n",
        "                         )\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(299),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)"
      ],
      "metadata": {
        "id": "_1Meb10AkVsQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda import is_available\n",
        "input_batch = input_tensor.unsqueeze(0)\n",
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  input_batch = input_batch.to('cuda')\n",
        "  model.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWg0o3kZoJkh",
        "outputId": "b70250c0-3850-4879-9e41-4f3c3a1bb659"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(), torch.profiler.profile(\n",
        "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]\n",
        ") as prof:\n",
        "\n",
        "  output = model(input_batch)\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-TMTlTWoWWc",
        "outputId": "37d9f208-f7f8-43e5-d31f-e34e8e315a2c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.3789e-04, 1.3163e-04, 7.5630e-05, 1.1083e-04, 9.6404e-05, 1.3945e-04,\n",
            "        2.1075e-04, 1.2519e-04, 1.0905e-04, 3.8484e-05, 9.4773e-05, 6.9633e-05,\n",
            "        1.0472e-04, 1.2593e-04, 2.7572e-04, 1.1175e-04, 7.3579e-05, 1.1755e-04,\n",
            "        1.5517e-04, 1.3978e-04, 1.8342e-04, 8.0105e-05, 1.3487e-04, 6.9928e-05,\n",
            "        1.1782e-04, 9.3183e-05, 9.3396e-05, 1.1701e-04, 1.1987e-04, 1.0378e-04,\n",
            "        1.4469e-04, 1.1704e-04, 4.3569e-04, 9.3473e-05, 3.1708e-04, 5.4145e-05,\n",
            "        1.9219e-04, 7.7250e-05, 9.0401e-05, 1.0144e-04, 1.6749e-04, 9.8975e-05,\n",
            "        1.3058e-04, 1.7978e-04, 1.0211e-04, 1.1947e-04, 7.4048e-05, 1.6887e-04,\n",
            "        1.3720e-04, 1.4427e-04, 1.0270e-04, 1.3691e-04, 1.5659e-04, 8.1339e-05,\n",
            "        3.1305e-04, 2.6110e-04, 5.6214e-05, 1.0892e-04, 1.0703e-04, 1.3314e-04,\n",
            "        1.0379e-04, 1.7033e-04, 7.4884e-05, 5.5784e-05, 1.0887e-04, 1.4640e-04,\n",
            "        1.5442e-04, 3.2324e-04, 1.7711e-04, 9.7884e-05, 1.0745e-04, 1.0405e-04,\n",
            "        4.1513e-05, 7.8393e-05, 1.4158e-04, 8.9499e-05, 8.7930e-05, 1.1980e-04,\n",
            "        1.2130e-04, 1.4422e-04, 7.2664e-05, 1.0982e-04, 1.3955e-04, 1.1245e-04,\n",
            "        2.7345e-04, 8.1630e-05, 8.7214e-05, 8.1318e-05, 1.5425e-04, 1.7110e-04,\n",
            "        1.4118e-04, 1.6119e-04, 1.8554e-04, 1.0313e-04, 8.7799e-05, 8.9123e-05,\n",
            "        2.7357e-04, 9.3545e-05, 8.5966e-05, 1.1280e-04, 1.3738e-04, 1.6158e-04,\n",
            "        1.4197e-04, 1.0698e-04, 1.8910e-04, 4.8384e-05, 5.7102e-05, 1.4921e-04,\n",
            "        2.2969e-04, 1.3530e-04, 9.2481e-05, 1.1511e-04, 1.1161e-04, 1.9278e-04,\n",
            "        1.2386e-04, 6.5334e-05, 5.4445e-05, 1.3554e-04, 5.1794e-05, 1.2266e-04,\n",
            "        1.4021e-04, 4.2960e-05, 1.1704e-04, 9.0309e-05, 8.8225e-05, 1.1028e-04,\n",
            "        6.1640e-05, 3.0059e-04, 2.0428e-04, 1.9992e-04, 1.3137e-04, 1.4122e-04,\n",
            "        2.1668e-04, 6.4144e-05, 1.6498e-04, 1.4233e-04, 2.1258e-04, 7.8264e-05,\n",
            "        5.6452e-05, 6.2896e-05, 8.9439e-05, 1.5364e-04, 5.5303e-05, 1.5147e-04,\n",
            "        3.0783e-04, 1.0719e-04, 1.6011e-04, 1.1468e-04, 1.2217e-04, 1.4054e-04,\n",
            "        8.8313e-05, 1.3968e-04, 1.3296e-03, 1.0400e-04, 2.9293e-04, 4.8759e-05,\n",
            "        1.1190e-04, 9.6276e-04, 6.0752e-05, 9.2139e-05, 5.0054e-05, 3.1633e-05,\n",
            "        3.9065e-05, 9.2205e-05, 1.0273e-04, 1.0457e-04, 9.5664e-05, 8.1959e-05,\n",
            "        1.0708e-04, 2.3750e-04, 9.4241e-05, 9.8035e-05, 5.0154e-05, 9.8013e-05,\n",
            "        4.3532e-04, 7.2511e-05, 6.6854e-05, 9.6538e-05, 9.9578e-05, 1.0188e-04,\n",
            "        4.9929e-05, 8.3605e-05, 8.7947e-05, 7.4680e-05, 7.0561e-05, 1.8282e-04,\n",
            "        2.6309e-04, 7.2201e-05, 5.8639e-05, 4.9878e-05, 7.7598e-05, 7.5406e-05,\n",
            "        1.4494e-04, 3.4571e-05, 7.4878e-05, 6.9518e-05, 3.8565e-05, 1.5405e-04,\n",
            "        7.4508e-05, 1.5597e-04, 2.1546e-04, 4.8604e-05, 4.8798e-05, 5.2085e-04,\n",
            "        7.3577e-05, 3.9061e-05, 6.5023e-05, 2.2183e-04, 6.5301e-05, 5.8235e-05,\n",
            "        7.9959e-05, 6.2658e-05, 9.5383e-05, 9.9897e-05, 2.2325e-04, 1.2132e-04,\n",
            "        1.8668e-04, 1.2824e-04, 1.1165e-04, 1.7592e-05, 1.0495e-04, 9.2807e-05,\n",
            "        1.4993e-03, 6.7850e-04, 4.6877e-04, 6.9094e-05, 1.6589e-04, 5.8123e-05,\n",
            "        1.6207e-04, 1.0674e-04, 3.5807e-04, 4.2645e-04, 3.3563e-04, 1.0953e-04,\n",
            "        9.3322e-05, 1.2578e-04, 2.0238e-04, 7.4732e-05, 1.0817e-04, 7.7360e-05,\n",
            "        2.4357e-04, 1.8592e-04, 1.0796e-04, 5.0871e-05, 8.4915e-05, 6.9439e-05,\n",
            "        7.6784e-05, 1.9430e-04, 3.0973e-03, 1.2960e-03, 5.4488e-04, 9.5718e-05,\n",
            "        9.8023e-05, 8.9066e-05, 9.8600e-05, 1.5666e-04, 2.0053e-04, 2.0865e-03,\n",
            "        8.2385e-01, 7.8747e-03, 1.4819e-03, 6.3878e-03, 8.6172e-05, 6.3148e-05,\n",
            "        9.0582e-05, 9.3243e-05, 8.4619e-05, 1.4092e-04, 7.8441e-05, 2.6809e-04,\n",
            "        1.2304e-02, 4.7932e-05, 7.9985e-05, 2.1654e-04, 1.6644e-04, 5.0570e-05,\n",
            "        1.0931e-04, 2.1297e-04, 9.4046e-05, 1.4192e-02, 1.5612e-04, 8.3996e-05,\n",
            "        5.5270e-05, 4.2677e-04, 6.2218e-05, 5.7350e-05, 4.1915e-05, 1.4657e-04,\n",
            "        1.1129e-04, 1.3403e-04, 7.4026e-05, 1.2206e-04, 8.1753e-05, 1.1236e-04,\n",
            "        4.3705e-05, 3.0996e-05, 2.2584e-04, 8.9821e-05, 5.9702e-05, 6.6557e-05,\n",
            "        6.9044e-05, 8.0268e-05, 1.1046e-04, 8.8761e-05, 9.8635e-05, 1.1450e-04,\n",
            "        1.5019e-04, 9.8471e-05, 1.4130e-04, 1.0092e-04, 9.3002e-05, 1.3743e-04,\n",
            "        1.0487e-04, 8.8093e-05, 1.5087e-04, 1.4661e-04, 7.0756e-05, 1.5266e-04,\n",
            "        1.0019e-04, 8.9877e-05, 1.7794e-04, 7.9738e-05, 6.1332e-05, 1.4459e-04,\n",
            "        3.3062e-05, 7.6429e-05, 1.3752e-04, 1.6584e-04, 2.8164e-04, 1.1311e-04,\n",
            "        9.8216e-05, 1.7547e-04, 3.1600e-04, 7.1528e-05, 1.1532e-04, 6.5687e-05,\n",
            "        4.6491e-05, 7.0886e-05, 1.1960e-04, 9.0590e-05, 1.1927e-04, 9.9092e-05,\n",
            "        9.3755e-05, 9.5763e-05, 8.8914e-05, 1.2156e-04, 2.0960e-04, 1.6328e-04,\n",
            "        9.6653e-05, 5.2254e-05, 6.9747e-05, 9.9854e-05, 7.7384e-05, 1.0219e-04,\n",
            "        9.0820e-05, 1.3552e-04, 1.0831e-04, 9.5192e-05, 2.5401e-04, 2.0472e-04,\n",
            "        3.9530e-05, 2.2288e-04, 5.6445e-05, 1.8255e-04, 5.4117e-05, 5.3253e-05,\n",
            "        2.7644e-05, 5.3367e-05, 5.3605e-05, 4.8436e-05, 1.4448e-04, 7.5397e-05,\n",
            "        4.7034e-05, 1.2617e-04, 8.8672e-05, 8.0801e-05, 4.4649e-05, 1.3134e-04,\n",
            "        7.1523e-05, 3.2104e-05, 5.4953e-05, 6.1036e-05, 5.6657e-05, 4.1905e-05,\n",
            "        3.2632e-05, 8.1589e-05, 4.4080e-05, 1.0690e-04, 7.7710e-05, 1.1003e-04,\n",
            "        8.2422e-05, 8.1855e-05, 1.4547e-04, 2.0493e-04, 9.5227e-05, 9.0778e-05,\n",
            "        1.8493e-04, 1.0937e-04, 1.9806e-04, 1.3970e-04, 1.1407e-04, 4.2206e-05,\n",
            "        2.0547e-04, 1.3798e-04, 1.2988e-04, 1.4218e-04, 8.5100e-05, 8.5006e-05,\n",
            "        1.5156e-04, 8.8605e-05, 6.4777e-05, 4.2265e-05, 6.6978e-05, 8.0290e-05,\n",
            "        7.7695e-05, 6.5445e-05, 6.3324e-05, 1.4366e-04, 7.3764e-05, 1.3444e-04,\n",
            "        9.6182e-05, 4.2716e-05, 9.3491e-05, 1.4009e-04, 7.2786e-05, 1.5183e-04,\n",
            "        1.3895e-04, 1.2550e-04, 9.4490e-05, 1.4432e-04, 9.9893e-05, 7.9236e-05,\n",
            "        1.5048e-04, 1.3881e-04, 5.6977e-05, 1.1664e-04, 1.1564e-04, 8.6143e-05,\n",
            "        6.9333e-05, 1.2793e-04, 2.0869e-04, 8.6650e-05, 1.7252e-04, 6.1625e-05,\n",
            "        1.2889e-04, 1.9012e-04, 5.3506e-05, 8.3790e-05, 1.8995e-04, 1.3785e-04,\n",
            "        9.8111e-05, 1.0324e-04, 1.4857e-04, 7.1200e-05, 9.4014e-05, 1.2759e-04,\n",
            "        1.9851e-04, 1.3801e-04, 1.0809e-04, 1.5958e-04, 1.2997e-04, 3.2863e-05,\n",
            "        2.3808e-04, 1.3638e-04, 1.6374e-04, 8.2100e-05, 6.3377e-05, 4.8518e-05,\n",
            "        1.8106e-04, 1.3192e-04, 1.6376e-04, 1.2828e-04, 1.2639e-04, 2.6574e-04,\n",
            "        5.2545e-05, 5.9563e-05, 4.4820e-04, 9.0643e-05, 2.8715e-04, 1.3936e-04,\n",
            "        1.0435e-04, 1.8083e-04, 4.3124e-05, 1.6870e-04, 1.6934e-04, 5.1399e-05,\n",
            "        1.5883e-04, 1.2264e-04, 1.6730e-04, 1.1784e-04, 1.1954e-04, 1.9027e-04,\n",
            "        6.5378e-05, 8.8203e-05, 1.0638e-04, 2.4741e-04, 1.5437e-04, 7.5246e-05,\n",
            "        1.0045e-04, 1.9100e-04, 6.2651e-05, 9.5121e-05, 1.9618e-04, 5.9070e-05,\n",
            "        5.6045e-05, 1.4401e-04, 1.2652e-04, 1.1912e-04, 1.3624e-04, 6.0251e-05,\n",
            "        1.5098e-04, 2.5691e-04, 1.2416e-04, 7.9121e-05, 1.3877e-04, 1.3033e-04,\n",
            "        4.6143e-05, 6.9147e-05, 5.7076e-05, 3.3399e-05, 9.2047e-05, 2.4921e-04,\n",
            "        3.3814e-04, 5.9347e-05, 1.0035e-04, 7.6443e-05, 6.4763e-05, 5.8407e-05,\n",
            "        8.0610e-05, 6.6059e-05, 1.6029e-04, 1.1538e-04, 1.6766e-04, 9.1680e-05,\n",
            "        1.1775e-04, 8.6828e-05, 4.3311e-05, 2.2743e-04, 1.1561e-04, 1.9959e-04,\n",
            "        1.0593e-04, 8.0767e-05, 6.8741e-05, 1.3585e-04, 1.8427e-04, 8.5826e-05,\n",
            "        1.6034e-04, 1.0730e-04, 5.9759e-05, 1.5501e-04, 1.0081e-04, 1.5739e-04,\n",
            "        1.0534e-04, 3.9192e-05, 1.0880e-04, 7.8521e-05, 1.2034e-04, 1.6677e-04,\n",
            "        1.2310e-04, 1.7814e-04, 9.2497e-05, 4.5738e-05, 2.1370e-04, 1.8028e-04,\n",
            "        1.4479e-04, 9.5273e-05, 1.2210e-04, 1.5333e-04, 8.7921e-05, 7.2575e-05,\n",
            "        1.0344e-04, 5.8679e-05, 1.1788e-04, 2.5330e-04, 1.0116e-04, 8.4435e-05,\n",
            "        1.6069e-04, 1.0416e-04, 1.7503e-04, 9.0930e-05, 1.2575e-04, 1.5500e-04,\n",
            "        1.0080e-04, 1.1768e-04, 2.2357e-04, 1.8615e-04, 5.6471e-05, 1.5928e-04,\n",
            "        7.1527e-05, 1.0970e-04, 1.9024e-04, 1.2266e-04, 2.2716e-04, 1.7793e-04,\n",
            "        1.8949e-04, 6.3696e-05, 2.9685e-04, 1.3345e-04, 6.4111e-05, 8.1671e-05,\n",
            "        6.0754e-05, 7.7922e-05, 1.2840e-04, 8.2812e-05, 2.9178e-04, 9.0174e-05,\n",
            "        3.0038e-04, 1.5257e-04, 1.9953e-04, 1.3730e-04, 1.3140e-04, 1.0673e-04,\n",
            "        1.1589e-04, 1.2166e-04, 6.5141e-05, 1.2233e-04, 4.6736e-05, 7.1222e-05,\n",
            "        6.7550e-05, 6.8862e-05, 7.0484e-05, 1.4674e-04, 2.0495e-04, 1.4957e-04,\n",
            "        1.2539e-04, 1.6214e-04, 1.5214e-04, 1.5255e-04, 2.0110e-04, 1.6230e-04,\n",
            "        1.3034e-04, 6.7662e-05, 8.1362e-05, 8.7017e-05, 2.4933e-05, 6.4827e-05,\n",
            "        1.4317e-04, 8.1907e-05, 9.8015e-05, 1.9795e-04, 9.5943e-05, 1.3835e-04,\n",
            "        7.7600e-05, 7.0602e-05, 1.0345e-04, 2.3191e-04, 2.0585e-04, 6.8991e-05,\n",
            "        8.1862e-05, 1.6392e-04, 9.4299e-05, 1.3195e-04, 1.6893e-04, 9.3728e-05,\n",
            "        1.0355e-04, 6.1197e-05, 1.7714e-04, 5.7766e-05, 8.8961e-05, 1.5015e-04,\n",
            "        7.3689e-05, 1.1495e-04, 1.9412e-04, 4.8885e-05, 6.9602e-05, 1.4030e-04,\n",
            "        7.7098e-05, 2.5881e-04, 6.8740e-05, 1.6080e-04, 1.5686e-04, 8.0340e-05,\n",
            "        3.1900e-04, 1.0057e-04, 1.6199e-04, 2.1298e-04, 1.9547e-04, 9.7691e-05,\n",
            "        1.3254e-04, 2.1542e-04, 2.0050e-04, 7.5994e-05, 2.8767e-04, 1.3153e-04,\n",
            "        1.5872e-04, 9.8518e-05, 7.9926e-05, 1.0590e-04, 9.8233e-05, 1.1165e-04,\n",
            "        1.7100e-04, 5.2128e-05, 6.4958e-05, 5.1861e-05, 7.7346e-05, 2.5882e-04,\n",
            "        9.7802e-05, 1.0296e-04, 1.7493e-04, 1.4803e-04, 7.1509e-05, 9.4490e-05,\n",
            "        4.4438e-05, 1.8407e-04, 6.5064e-05, 2.5496e-05, 2.2912e-04, 1.2174e-04,\n",
            "        8.3156e-05, 6.0666e-05, 1.3048e-04, 1.1163e-04, 1.3184e-04, 3.1956e-05,\n",
            "        1.5786e-04, 1.2058e-04, 8.1124e-05, 1.6981e-04, 2.1138e-04, 2.0331e-04,\n",
            "        1.3662e-04, 5.9801e-05, 1.8200e-04, 2.6542e-04, 3.0221e-04, 9.9845e-05,\n",
            "        1.5793e-04, 1.3082e-04, 4.4443e-05, 2.6401e-04, 7.0088e-05, 2.3055e-04,\n",
            "        1.0993e-04, 1.7376e-04, 1.0315e-04, 6.9110e-05, 9.5015e-05, 1.1427e-04,\n",
            "        1.9975e-04, 6.2006e-05, 2.4755e-04, 1.0674e-04, 9.3994e-05, 5.0497e-05,\n",
            "        1.0199e-04, 7.1729e-05, 3.0899e-04, 8.2024e-05, 7.4708e-05, 1.4032e-04,\n",
            "        4.9470e-05, 8.2005e-05, 1.0837e-04, 6.4223e-05, 4.8608e-05, 7.0058e-05,\n",
            "        1.0580e-04, 1.0470e-04, 2.4923e-04, 8.1969e-05, 8.1740e-05, 6.2662e-05,\n",
            "        1.1540e-04, 2.0908e-04, 7.0996e-05, 1.6991e-04, 6.5767e-05, 1.5084e-04,\n",
            "        2.1160e-04, 2.1199e-04, 1.0566e-04, 9.4552e-05, 1.0635e-04, 8.7335e-05,\n",
            "        1.3286e-04, 1.1957e-04, 1.4071e-04, 1.1590e-04, 6.9578e-05, 1.1861e-04,\n",
            "        2.7826e-04, 1.0763e-04, 1.2882e-04, 6.9899e-05, 1.3954e-04, 1.0994e-04,\n",
            "        8.2220e-05, 7.6260e-05, 2.4670e-04, 1.1906e-04, 1.2292e-04, 1.6211e-04,\n",
            "        1.9407e-04, 8.0322e-05, 6.7745e-05, 7.9686e-05, 7.1091e-05, 5.2398e-05,\n",
            "        9.5090e-05, 1.8802e-04, 1.2781e-04, 1.1503e-04, 1.4747e-04, 1.4106e-04,\n",
            "        6.0509e-05, 1.1891e-04, 4.1991e-05, 5.0752e-05, 1.2809e-04, 1.4398e-04,\n",
            "        2.7699e-04, 7.5884e-05, 1.3417e-04, 2.9568e-04, 2.0827e-04, 2.0823e-04,\n",
            "        1.4964e-04, 1.1671e-04, 9.0126e-05, 7.6295e-05, 8.5880e-05, 5.9993e-05,\n",
            "        1.2891e-04, 8.0204e-05, 1.4202e-04, 1.8111e-04, 1.3963e-04, 9.4848e-05,\n",
            "        1.9303e-04, 6.2052e-05, 9.6571e-05, 1.5938e-04, 2.2634e-04, 1.3481e-04,\n",
            "        7.4877e-05, 7.5932e-05, 2.9892e-04, 1.2380e-04, 2.5970e-04, 8.9123e-05,\n",
            "        1.0363e-04, 1.0538e-04, 1.1671e-04, 2.9257e-04, 3.9147e-04, 9.2042e-05,\n",
            "        7.7660e-05, 5.7550e-05, 1.3069e-04, 9.9113e-05, 8.2578e-05, 5.0687e-05,\n",
            "        3.8089e-04, 2.0814e-04, 1.5194e-04, 7.2544e-05, 6.7090e-05, 2.1042e-04,\n",
            "        2.0922e-04, 1.0943e-04, 1.3765e-04, 1.3999e-04, 8.3047e-05, 1.3873e-04,\n",
            "        5.1307e-05, 4.3942e-05, 6.4611e-05, 1.0395e-04, 2.0588e-05, 8.5195e-05,\n",
            "        1.6049e-04, 8.3906e-05, 1.2139e-04, 1.0251e-04, 8.8951e-05, 7.5162e-05,\n",
            "        8.3283e-05, 4.3266e-05, 1.9234e-04, 1.2097e-04, 4.2794e-05, 7.3660e-05,\n",
            "        1.0775e-04, 4.1266e-05, 1.0645e-04, 1.2244e-04, 1.3647e-04, 9.5770e-05,\n",
            "        1.2988e-04, 1.8801e-04, 1.1110e-04, 1.2521e-04, 1.2468e-04, 2.2765e-04,\n",
            "        5.9139e-05, 8.2254e-05, 2.1957e-04, 1.5286e-04, 2.1870e-04, 1.4502e-04,\n",
            "        1.2217e-04, 4.4683e-05, 1.6477e-04, 5.2579e-05, 7.7208e-05, 7.5506e-05,\n",
            "        1.0120e-04, 1.5572e-04, 1.1610e-04, 9.8177e-05, 2.1770e-04, 1.5481e-04,\n",
            "        1.9548e-04, 2.8968e-04, 1.8207e-04, 2.4922e-04, 1.1395e-04, 7.2502e-05,\n",
            "        1.2013e-04, 1.3012e-04, 1.0898e-04, 1.5224e-04, 1.9599e-04, 2.0053e-04,\n",
            "        6.7755e-05, 1.3591e-04, 9.0142e-05, 7.8266e-05, 1.1173e-04, 1.9823e-04,\n",
            "        2.1871e-04, 5.5641e-05, 6.0286e-05, 6.3268e-05, 5.5981e-05, 1.4486e-04,\n",
            "        9.1151e-05, 1.5084e-04, 8.4302e-05, 2.5276e-04, 1.6155e-04, 2.0768e-04,\n",
            "        2.2738e-04, 1.0886e-04, 2.5628e-04, 2.3372e-04, 1.4125e-04, 6.6199e-05,\n",
            "        1.4856e-04, 1.4160e-04, 2.0076e-04, 1.3123e-04, 2.1294e-04, 1.3945e-04,\n",
            "        1.6473e-04, 1.5033e-04, 1.7234e-04, 1.4929e-04, 1.2784e-04, 1.0017e-04,\n",
            "        9.5561e-05, 1.6914e-04, 1.4045e-04, 1.1522e-04, 8.5927e-05, 8.6531e-05,\n",
            "        1.5293e-04, 1.3893e-04, 5.1191e-05, 1.5067e-04, 1.1576e-04, 1.1115e-04,\n",
            "        2.1050e-04, 1.2091e-04, 1.3931e-04, 9.0170e-05, 1.3954e-04, 4.3246e-04,\n",
            "        2.4617e-04, 1.5332e-04, 8.9369e-05, 9.2457e-05, 9.4682e-05, 1.3860e-04,\n",
            "        8.5710e-05, 1.2257e-04, 1.3560e-04, 7.8325e-05, 1.4312e-04, 1.0707e-04,\n",
            "        8.6765e-05, 7.9838e-05, 9.7519e-05, 4.7767e-05, 4.9246e-05, 7.6463e-05,\n",
            "        1.0947e-04, 5.1298e-05, 6.9876e-05, 1.3237e-04], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Eem9hrUo8ut",
        "outputId": "58467a0e-4e46-457b-be4f-dcd52ce659d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-04 09:31:48--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10472 (10K) [text/plain]\n",
            "Saving to: ‘imagenet_classes.txt.1’\n",
            "\n",
            "\rimagenet_classes.tx   0%[                    ]       0  --.-KB/s               \rimagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-12-04 09:31:48 (109 MB/s) - ‘imagenet_classes.txt.1’ saved [10472/10472]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('imagenet_classes.txt',\"r\") as f:\n",
        "  categories = [s.strip() for s in f.readlines()]\n",
        "print(categories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVypqshWoi8p",
        "outputId": "037df915-e413-4d74-b9ae-acf803d5adf4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead', 'electric ray', 'stingray', 'cock', 'hen', 'ostrich', 'brambling', 'goldfinch', 'house finch', 'junco', 'indigo bunting', 'robin', 'bulbul', 'jay', 'magpie', 'chickadee', 'water ouzel', 'kite', 'bald eagle', 'vulture', 'great grey owl', 'European fire salamander', 'common newt', 'eft', 'spotted salamander', 'axolotl', 'bullfrog', 'tree frog', 'tailed frog', 'loggerhead', 'leatherback turtle', 'mud turtle', 'terrapin', 'box turtle', 'banded gecko', 'common iguana', 'American chameleon', 'whiptail', 'agama', 'frilled lizard', 'alligator lizard', 'Gila monster', 'green lizard', 'African chameleon', 'Komodo dragon', 'African crocodile', 'American alligator', 'triceratops', 'thunder snake', 'ringneck snake', 'hognose snake', 'green snake', 'king snake', 'garter snake', 'water snake', 'vine snake', 'night snake', 'boa constrictor', 'rock python', 'Indian cobra', 'green mamba', 'sea snake', 'horned viper', 'diamondback', 'sidewinder', 'trilobite', 'harvestman', 'scorpion', 'black and gold garden spider', 'barn spider', 'garden spider', 'black widow', 'tarantula', 'wolf spider', 'tick', 'centipede', 'black grouse', 'ptarmigan', 'ruffed grouse', 'prairie chicken', 'peacock', 'quail', 'partridge', 'African grey', 'macaw', 'sulphur-crested cockatoo', 'lorikeet', 'coucal', 'bee eater', 'hornbill', 'hummingbird', 'jacamar', 'toucan', 'drake', 'red-breasted merganser', 'goose', 'black swan', 'tusker', 'echidna', 'platypus', 'wallaby', 'koala', 'wombat', 'jellyfish', 'sea anemone', 'brain coral', 'flatworm', 'nematode', 'conch', 'snail', 'slug', 'sea slug', 'chiton', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'fiddler crab', 'king crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'white stork', 'black stork', 'spoonbill', 'flamingo', 'little blue heron', 'American egret', 'bittern', 'crane', 'limpkin', 'European gallinule', 'American coot', 'bustard', 'ruddy turnstone', 'red-backed sandpiper', 'redshank', 'dowitcher', 'oystercatcher', 'pelican', 'king penguin', 'albatross', 'grey whale', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese spaniel', 'Maltese dog', 'Pekinese', 'Shih-Tzu', 'Blenheim spaniel', 'papillon', 'toy terrier', 'Rhodesian ridgeback', 'Afghan hound', 'basset', 'beagle', 'bloodhound', 'bluetick', 'black-and-tan coonhound', 'Walker hound', 'English foxhound', 'redbone', 'borzoi', 'Irish wolfhound', 'Italian greyhound', 'whippet', 'Ibizan hound', 'Norwegian elkhound', 'otterhound', 'Saluki', 'Scottish deerhound', 'Weimaraner', 'Staffordshire bullterrier', 'American Staffordshire terrier', 'Bedlington terrier', 'Border terrier', 'Kerry blue terrier', 'Irish terrier', 'Norfolk terrier', 'Norwich terrier', 'Yorkshire terrier', 'wire-haired fox terrier', 'Lakeland terrier', 'Sealyham terrier', 'Airedale', 'cairn', 'Australian terrier', 'Dandie Dinmont', 'Boston bull', 'miniature schnauzer', 'giant schnauzer', 'standard schnauzer', 'Scotch terrier', 'Tibetan terrier', 'silky terrier', 'soft-coated wheaten terrier', 'West Highland white terrier', 'Lhasa', 'flat-coated retriever', 'curly-coated retriever', 'golden retriever', 'Labrador retriever', 'Chesapeake Bay retriever', 'German short-haired pointer', 'vizsla', 'English setter', 'Irish setter', 'Gordon setter', 'Brittany spaniel', 'clumber', 'English springer', 'Welsh springer spaniel', 'cocker spaniel', 'Sussex spaniel', 'Irish water spaniel', 'kuvasz', 'schipperke', 'groenendael', 'malinois', 'briard', 'kelpie', 'komondor', 'Old English sheepdog', 'Shetland sheepdog', 'collie', 'Border collie', 'Bouvier des Flandres', 'Rottweiler', 'German shepherd', 'Doberman', 'miniature pinscher', 'Greater Swiss Mountain dog', 'Bernese mountain dog', 'Appenzeller', 'EntleBucher', 'boxer', 'bull mastiff', 'Tibetan mastiff', 'French bulldog', 'Great Dane', 'Saint Bernard', 'Eskimo dog', 'malamute', 'Siberian husky', 'dalmatian', 'affenpinscher', 'basenji', 'pug', 'Leonberg', 'Newfoundland', 'Great Pyrenees', 'Samoyed', 'Pomeranian', 'chow', 'keeshond', 'Brabancon griffon', 'Pembroke', 'Cardigan', 'toy poodle', 'miniature poodle', 'standard poodle', 'Mexican hairless', 'timber wolf', 'white wolf', 'red wolf', 'coyote', 'dingo', 'dhole', 'African hunting dog', 'hyena', 'red fox', 'kit fox', 'Arctic fox', 'grey fox', 'tabby', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian cat', 'cougar', 'lynx', 'leopard', 'snow leopard', 'jaguar', 'lion', 'tiger', 'cheetah', 'brown bear', 'American black bear', 'ice bear', 'sloth bear', 'mongoose', 'meerkat', 'tiger beetle', 'ladybug', 'ground beetle', 'long-horned beetle', 'leaf beetle', 'dung beetle', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'cricket', 'walking stick', 'cockroach', 'mantis', 'cicada', 'leafhopper', 'lacewing', 'dragonfly', 'damselfly', 'admiral', 'ringlet', 'monarch', 'cabbage butterfly', 'sulphur butterfly', 'lycaenid', 'starfish', 'sea urchin', 'sea cucumber', 'wood rabbit', 'hare', 'Angora', 'hamster', 'porcupine', 'fox squirrel', 'marmot', 'beaver', 'guinea pig', 'sorrel', 'zebra', 'hog', 'wild boar', 'warthog', 'hippopotamus', 'ox', 'water buffalo', 'bison', 'ram', 'bighorn', 'ibex', 'hartebeest', 'impala', 'gazelle', 'Arabian camel', 'llama', 'weasel', 'mink', 'polecat', 'black-footed ferret', 'otter', 'skunk', 'badger', 'armadillo', 'three-toed sloth', 'orangutan', 'gorilla', 'chimpanzee', 'gibbon', 'siamang', 'guenon', 'patas', 'baboon', 'macaque', 'langur', 'colobus', 'proboscis monkey', 'marmoset', 'capuchin', 'howler monkey', 'titi', 'spider monkey', 'squirrel monkey', 'Madagascar cat', 'indri', 'Indian elephant', 'African elephant', 'lesser panda', 'giant panda', 'barracouta', 'eel', 'coho', 'rock beauty', 'anemone fish', 'sturgeon', 'gar', 'lionfish', 'puffer', 'abacus', 'abaya', 'academic gown', 'accordion', 'acoustic guitar', 'aircraft carrier', 'airliner', 'airship', 'altar', 'ambulance', 'amphibian', 'analog clock', 'apiary', 'apron', 'ashcan', 'assault rifle', 'backpack', 'bakery', 'balance beam', 'balloon', 'ballpoint', 'Band Aid', 'banjo', 'bannister', 'barbell', 'barber chair', 'barbershop', 'barn', 'barometer', 'barrel', 'barrow', 'baseball', 'basketball', 'bassinet', 'bassoon', 'bathing cap', 'bath towel', 'bathtub', 'beach wagon', 'beacon', 'beaker', 'bearskin', 'beer bottle', 'beer glass', 'bell cote', 'bib', 'bicycle-built-for-two', 'bikini', 'binder', 'binoculars', 'birdhouse', 'boathouse', 'bobsled', 'bolo tie', 'bonnet', 'bookcase', 'bookshop', 'bottlecap', 'bow', 'bow tie', 'brass', 'brassiere', 'breakwater', 'breastplate', 'broom', 'bucket', 'buckle', 'bulletproof vest', 'bullet train', 'butcher shop', 'cab', 'caldron', 'candle', 'cannon', 'canoe', 'can opener', 'cardigan', 'car mirror', 'carousel', \"carpenter's kit\", 'carton', 'car wheel', 'cash machine', 'cassette', 'cassette player', 'castle', 'catamaran', 'CD player', 'cello', 'cellular telephone', 'chain', 'chainlink fence', 'chain mail', 'chain saw', 'chest', 'chiffonier', 'chime', 'china cabinet', 'Christmas stocking', 'church', 'cinema', 'cleaver', 'cliff dwelling', 'cloak', 'clog', 'cocktail shaker', 'coffee mug', 'coffeepot', 'coil', 'combination lock', 'computer keyboard', 'confectionery', 'container ship', 'convertible', 'corkscrew', 'cornet', 'cowboy boot', 'cowboy hat', 'cradle', 'crane', 'crash helmet', 'crate', 'crib', 'Crock Pot', 'croquet ball', 'crutch', 'cuirass', 'dam', 'desk', 'desktop computer', 'dial telephone', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishrag', 'dishwasher', 'disk brake', 'dock', 'dogsled', 'dome', 'doormat', 'drilling platform', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric guitar', 'electric locomotive', 'entertainment center', 'envelope', 'espresso maker', 'face powder', 'feather boa', 'file', 'fireboat', 'fire engine', 'fire screen', 'flagpole', 'flute', 'folding chair', 'football helmet', 'forklift', 'fountain', 'fountain pen', 'four-poster', 'freight car', 'French horn', 'frying pan', 'fur coat', 'garbage truck', 'gasmask', 'gas pump', 'goblet', 'go-kart', 'golf ball', 'golfcart', 'gondola', 'gong', 'gown', 'grand piano', 'greenhouse', 'grille', 'grocery store', 'guillotine', 'hair slide', 'hair spray', 'half track', 'hammer', 'hamper', 'hand blower', 'hand-held computer', 'handkerchief', 'hard disc', 'harmonica', 'harp', 'harvester', 'hatchet', 'holster', 'home theater', 'honeycomb', 'hook', 'hoopskirt', 'horizontal bar', 'horse cart', 'hourglass', 'iPod', 'iron', \"jack-o'-lantern\", 'jean', 'jeep', 'jersey', 'jigsaw puzzle', 'jinrikisha', 'joystick', 'kimono', 'knee pad', 'knot', 'lab coat', 'ladle', 'lampshade', 'laptop', 'lawn mower', 'lens cap', 'letter opener', 'library', 'lifeboat', 'lighter', 'limousine', 'liner', 'lipstick', 'Loafer', 'lotion', 'loudspeaker', 'loupe', 'lumbermill', 'magnetic compass', 'mailbag', 'mailbox', 'maillot', 'maillot', 'manhole cover', 'maraca', 'marimba', 'mask', 'matchstick', 'maypole', 'maze', 'measuring cup', 'medicine chest', 'megalith', 'microphone', 'microwave', 'military uniform', 'milk can', 'minibus', 'miniskirt', 'minivan', 'missile', 'mitten', 'mixing bowl', 'mobile home', 'Model T', 'modem', 'monastery', 'monitor', 'moped', 'mortar', 'mortarboard', 'mosque', 'mosquito net', 'motor scooter', 'mountain bike', 'mountain tent', 'mouse', 'mousetrap', 'moving van', 'muzzle', 'nail', 'neck brace', 'necklace', 'nipple', 'notebook', 'obelisk', 'oboe', 'ocarina', 'odometer', 'oil filter', 'organ', 'oscilloscope', 'overskirt', 'oxcart', 'oxygen mask', 'packet', 'paddle', 'paddlewheel', 'padlock', 'paintbrush', 'pajama', 'palace', 'panpipe', 'paper towel', 'parachute', 'parallel bars', 'park bench', 'parking meter', 'passenger car', 'patio', 'pay-phone', 'pedestal', 'pencil box', 'pencil sharpener', 'perfume', 'Petri dish', 'photocopier', 'pick', 'pickelhaube', 'picket fence', 'pickup', 'pier', 'piggy bank', 'pill bottle', 'pillow', 'ping-pong ball', 'pinwheel', 'pirate', 'pitcher', 'plane', 'planetarium', 'plastic bag', 'plate rack', 'plow', 'plunger', 'Polaroid camera', 'pole', 'police van', 'poncho', 'pool table', 'pop bottle', 'pot', \"potter's wheel\", 'power drill', 'prayer rug', 'printer', 'prison', 'projectile', 'projector', 'puck', 'punching bag', 'purse', 'quill', 'quilt', 'racer', 'racket', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'reel', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rocking chair', 'rotisserie', 'rubber eraser', 'rugby ball', 'rule', 'running shoe', 'safe', 'safety pin', 'saltshaker', 'sandal', 'sarong', 'sax', 'scabbard', 'scale', 'school bus', 'schooner', 'scoreboard', 'screen', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shield', 'shoe shop', 'shoji', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'shower curtain', 'ski', 'ski mask', 'sleeping bag', 'slide rule', 'sliding door', 'slot', 'snorkel', 'snowmobile', 'snowplow', 'soap dispenser', 'soccer ball', 'sock', 'solar dish', 'sombrero', 'soup bowl', 'space bar', 'space heater', 'space shuttle', 'spatula', 'speedboat', 'spider web', 'spindle', 'sports car', 'spotlight', 'stage', 'steam locomotive', 'steel arch bridge', 'steel drum', 'stethoscope', 'stole', 'stone wall', 'stopwatch', 'stove', 'strainer', 'streetcar', 'stretcher', 'studio couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglass', 'sunglasses', 'sunscreen', 'suspension bridge', 'swab', 'sweatshirt', 'swimming trunks', 'swing', 'switch', 'syringe', 'table lamp', 'tank', 'tape player', 'teapot', 'teddy', 'television', 'tennis ball', 'thatch', 'theater curtain', 'thimble', 'thresher', 'throne', 'tile roof', 'toaster', 'tobacco shop', 'toilet seat', 'torch', 'totem pole', 'tow truck', 'toyshop', 'tractor', 'trailer truck', 'tray', 'trench coat', 'tricycle', 'trimaran', 'tripod', 'triumphal arch', 'trolleybus', 'trombone', 'tub', 'turnstile', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright', 'vacuum', 'vase', 'vault', 'velvet', 'vending machine', 'vestment', 'viaduct', 'violin', 'volleyball', 'waffle iron', 'wall clock', 'wallet', 'wardrobe', 'warplane', 'washbasin', 'washer', 'water bottle', 'water jug', 'water tower', 'whiskey jug', 'whistle', 'wig', 'window screen', 'window shade', 'Windsor tie', 'wine bottle', 'wing', 'wok', 'wooden spoon', 'wool', 'worm fence', 'wreck', 'yawl', 'yurt', 'web site', 'comic book', 'crossword puzzle', 'street sign', 'traffic light', 'book jacket', 'menu', 'plate', 'guacamole', 'consomme', 'hot pot', 'trifle', 'ice cream', 'ice lolly', 'French loaf', 'bagel', 'pretzel', 'cheeseburger', 'hotdog', 'mashed potato', 'head cabbage', 'broccoli', 'cauliflower', 'zucchini', 'spaghetti squash', 'acorn squash', 'butternut squash', 'cucumber', 'artichoke', 'bell pepper', 'cardoon', 'mushroom', 'Granny Smith', 'strawberry', 'orange', 'lemon', 'fig', 'pineapple', 'banana', 'jackfruit', 'custard apple', 'pomegranate', 'hay', 'carbonara', 'chocolate sauce', 'dough', 'meat loaf', 'pizza', 'potpie', 'burrito', 'red wine', 'espresso', 'cup', 'eggnog', 'alp', 'bubble', 'cliff', 'coral reef', 'geyser', 'lakeside', 'promontory', 'sandbar', 'seashore', 'valley', 'volcano', 'ballplayer', 'groom', 'scuba diver', 'rapeseed', 'daisy', \"yellow lady's slipper\", 'corn', 'acorn', 'hip', 'buckeye', 'coral fungus', 'agaric', 'gyromitra', 'stinkhorn', 'earthstar', 'hen-of-the-woods', 'bolete', 'ear', 'toilet tissue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "for i in range(top5_prob.size(0)):\n",
        "  category = categories[top5_catid[i]]\n",
        "  probability = top5_prob[i].item()\n",
        "  print(f\"category: {category}, Probability {probability:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkauE5pjo0jw",
        "outputId": "49552bbf-f62c-46c3-d4f6-49bfda041a0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "category: Samoyed, Probability 0.8238\n",
            "category: Arctic fox, Probability 0.0142\n",
            "category: white wolf, Probability 0.0123\n",
            "category: Pomeranian, Probability 0.0079\n",
            "category: keeshond, Probability 0.0064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages().table(sort_by='cuda_time_total'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5laGct_hpcXd",
        "outputId": "e3c431e5-c1f6-4873-d9b2-07e5460c5640"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         1.36%     232.746us        43.78%       7.468ms      79.442us       0.000us         0.00%      10.881ms     115.760us            94  \n",
            "                                      aten::convolution         2.42%     412.693us        42.42%       7.235ms      76.966us       0.000us         0.00%      10.881ms     115.760us            94  \n",
            "                                     aten::_convolution         3.28%     560.125us        40.00%       6.822ms      72.576us       0.000us         0.00%      10.881ms     115.760us            94  \n",
            "                                aten::cudnn_convolution        22.24%       3.793ms        36.71%       6.262ms      66.617us      10.881ms        87.55%      10.881ms     115.760us            94  \n",
            "                                  volta_gcgemm_32x32_nt         0.00%       0.000us         0.00%       0.000us       0.000us       1.900ms        15.28%       1.900ms      73.063us            26  \n",
            "void fft1d_r2c_32<float, float, float2, true, false>...         0.00%       0.000us         0.00%       0.000us       0.000us       1.439ms        11.58%       1.439ms      55.340us            26  \n",
            "_5x_cudnn_volta_scudnn_winograd_128x128_ldg1_ldg4_re...         0.00%       0.000us         0.00%       0.000us       0.000us       1.346ms        10.83%       1.346ms     149.518us             9  \n",
            "void implicit_convolve_sgemm<float, float, 128, 5, 5...         0.00%       0.000us         0.00%       0.000us       0.000us       1.292ms        10.39%       1.292ms     161.456us             8  \n",
            "_5x_cudnn_volta_scudnn_128x32_sliced1x4_ldg4_relu_ex...         0.00%       0.000us         0.00%       0.000us       0.000us     977.668us         7.87%     977.668us     244.417us             4  \n",
            "                         volta_sgemm_64x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us     753.803us         6.06%     753.803us      39.674us            19  \n",
            "                                  volta_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     693.326us         5.58%     693.326us      57.777us            12  \n",
            "                                       aten::batch_norm         1.78%     303.681us        32.64%       5.566ms      59.217us       0.000us         0.00%     559.979us       5.957us            94  \n",
            "                           aten::_batch_norm_impl_index         1.83%     311.303us        30.86%       5.263ms      55.986us       0.000us         0.00%     559.979us       5.957us            94  \n",
            "                                 aten::cudnn_batch_norm        11.97%       2.042ms        29.03%       4.951ms      52.675us     559.979us         4.51%     559.979us       5.957us            94  \n",
            "void cudnn::bn_fw_inf_1C11_kernel_NCHW<float, float,...         0.00%       0.000us         0.00%       0.000us       0.000us     559.979us         4.51%     559.979us       6.292us            89  \n",
            "void precomputed_convolve_sgemm<float, 1024, 5, 5, 4...         0.00%       0.000us         0.00%       0.000us       0.000us     507.027us         4.08%     507.027us     169.009us             3  \n",
            "void fft1d_r2c_32<float, float, float2, false, false...         0.00%       0.000us         0.00%       0.000us       0.000us     488.658us         3.93%     488.658us      37.589us            13  \n",
            "                                       aten::avg_pool2d         0.63%     106.853us         1.01%     172.255us      19.139us     412.149us         3.32%     412.149us      45.794us             9  \n",
            "void at::native::(anonymous namespace)::avg_pool2d_o...         0.00%       0.000us         0.00%       0.000us       0.000us     412.149us         3.32%     412.149us      45.794us             9  \n",
            "                                            aten::relu_         2.45%     417.117us        12.37%       2.109ms      22.438us       0.000us         0.00%     343.129us       3.650us            94  \n",
            "                                       aten::clamp_min_         5.79%     987.224us         9.92%       1.692ms      18.001us     343.129us         2.76%     343.129us       3.650us            94  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     343.129us         2.76%     343.129us       3.855us            89  \n",
            "void fft1d_r2c_32<float, float, float2, false, true>...         0.00%       0.000us         0.00%       0.000us       0.000us     334.355us         2.69%     334.355us      25.720us            13  \n",
            "void cudnn::winograd::generateWinogradTilesKernel<0,...         0.00%       0.000us         0.00%       0.000us       0.000us     218.460us         1.76%     218.460us      24.273us             9  \n",
            "                                   volta_sgemm_64x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     198.555us         1.60%     198.555us      66.185us             3  \n",
            "                         volta_sgemm_32x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us     167.929us         1.35%     167.929us      41.982us             4  \n",
            "void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     147.901us         1.19%     147.901us       7.784us            19  \n",
            "                                              aten::cat         1.57%     267.773us         2.30%     392.307us      24.519us     135.319us         1.09%     135.319us       8.457us            16  \n",
            "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     135.319us         1.09%     135.319us       9.021us            15  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us     115.325us         0.93%     115.325us      14.416us             8  \n",
            "void fft1d_c2r_32<float2, float, float, false, true,...         0.00%       0.000us         0.00%       0.000us       0.000us      97.438us         0.78%      97.438us       7.495us            13  \n",
            "void fft1d_c2r_32<float2, float, float, false, true,...         0.00%       0.000us         0.00%       0.000us       0.000us      97.020us         0.78%      97.020us       7.463us            13  \n",
            "                                  volta_sgemm_32x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us      57.439us         0.46%      57.439us      57.439us             1  \n",
            "                                           aten::linear         0.05%       7.813us         0.61%     104.113us     104.113us       0.000us         0.00%      42.687us      42.687us             1  \n",
            "                                            aten::addmm         0.41%      69.132us         0.49%      82.759us      82.759us      42.687us         0.34%      42.687us      42.687us             1  \n",
            "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      42.687us         0.34%      42.687us      42.687us             1  \n",
            "                                       aten::max_pool2d         0.11%      18.185us         0.69%     117.132us      29.283us       0.000us         0.00%      39.840us       9.960us             4  \n",
            "                          aten::max_pool2d_with_indices         0.40%      67.594us         0.58%      98.947us      24.737us      39.840us         0.32%      39.840us       9.960us             4  \n",
            "void at::native::(anonymous namespace)::max_pool_for...         0.00%       0.000us         0.00%       0.000us       0.000us      39.840us         0.32%      39.840us      19.920us             2  \n",
            "void cudnn::engines_precompiled::nhwcToNchwKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us      22.783us         0.18%      22.783us       5.696us             4  \n",
            "void cask__5x_cudnn::computeOffsetsKernel<false, fal...         0.00%       0.000us         0.00%       0.000us       0.000us      15.263us         0.12%      15.263us       3.816us             4  \n",
            "                              aten::adaptive_avg_pool2d         0.05%       7.982us         0.39%      65.732us      65.732us       0.000us         0.00%      14.943us      14.943us             1  \n",
            "                                             aten::mean         0.25%      43.404us         0.34%      57.750us      57.750us      14.943us         0.12%      14.943us      14.943us             1  \n",
            "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      14.943us         0.12%      14.943us      14.943us             1  \n",
            "void cudnn::cnn::kern_precompute_indices<false>(int*...         0.00%       0.000us         0.00%       0.000us       0.000us       8.896us         0.07%       8.896us       2.965us             3  \n",
            "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       3.840us         0.03%       3.840us       0.768us             5  \n",
            "                                           aten::select         0.30%      50.546us         0.38%      65.225us      21.742us       0.000us         0.00%       0.000us       0.000us             3  \n",
            "                                       aten::as_strided         0.13%      21.460us         0.13%      21.460us       3.066us       0.000us         0.00%       0.000us       0.000us             7  \n",
            "                                        aten::unsqueeze         0.09%      16.111us         0.12%      19.850us       6.617us       0.000us         0.00%       0.000us       0.000us             3  \n",
            "                                              aten::mul         0.77%     131.257us         5.02%     856.822us     285.607us       0.000us         0.00%       0.000us       0.000us             3  \n",
            "                                           Unrecognized         3.75%     639.072us         3.75%     639.072us     639.072us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                       cudaLaunchKernel        20.45%       3.487ms        20.45%       3.487ms       7.784us       0.000us         0.00%       0.000us       0.000us           448  \n",
            "                                              aten::add         0.31%      53.674us         0.47%      79.420us      26.473us       0.000us         0.00%       0.000us       0.000us             3  \n",
            "                                    cudaPeekAtLastError         0.05%       7.843us         0.05%       7.843us       0.231us       0.000us         0.00%       0.000us       0.000us            34  \n",
            "                                        cudaMemsetAsync         0.29%      48.673us         0.29%      48.673us       4.867us       0.000us         0.00%       0.000us       0.000us            10  \n",
            "                                       aten::empty_like         1.47%     250.194us         5.06%     862.938us       9.180us       0.000us         0.00%       0.000us       0.000us            94  \n",
            "                                            aten::empty         9.48%       1.617ms         9.48%       1.617ms       4.299us       0.000us         0.00%       0.000us       0.000us           376  \n",
            "                                             aten::view         1.28%     218.514us         1.28%     218.514us       2.300us       0.000us         0.00%       0.000us       0.000us            95  \n",
            "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.20%      33.537us         0.20%      33.537us       2.395us       0.000us         0.00%       0.000us       0.000us            14  \n",
            "                                        cudaEventRecord         1.17%     199.329us         1.17%     199.329us       0.767us       0.000us         0.00%       0.000us       0.000us           260  \n",
            "                                    cudaStreamWaitEvent         1.42%     242.408us         1.42%     242.408us       0.548us       0.000us         0.00%       0.000us       0.000us           442  \n",
            "                                  cudaStreamIsCapturing         0.01%       1.888us         0.01%       1.888us       1.888us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                             cudaMalloc         2.03%     345.417us         2.03%     345.417us     345.417us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                          aten::dropout         0.02%       2.901us         0.02%       2.901us       2.901us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                          aten::flatten         0.03%       4.888us         0.07%      11.681us      11.681us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                                aten::t         0.04%       6.670us         0.08%      13.541us      13.541us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                        aten::transpose         0.02%       3.829us         0.04%       6.871us       6.871us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                  cudaDeviceSynchronize         0.15%      25.273us         0.15%      25.273us      25.273us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 17.056ms\n",
            "Self CUDA time total: 12.430ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NUR6ZI-rQ6c"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}